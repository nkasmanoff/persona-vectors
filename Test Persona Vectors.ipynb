{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a829ea3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/noahkasmanoff/anaconda3/envs/torch_env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from typing import List, Tuple\n",
    "import numpy as np\n",
    "\n",
    "class PersonaVectorExtractor:\n",
    "    \"\"\"\n",
    "    Extracts persona vectors from LLM activation space by comparing\n",
    "    activations when the model exhibits vs doesn't exhibit a trait.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"gpt2\"):\n",
    "        \"\"\"Initialize with a language model.\"\"\"\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        # Hook to capture activations\n",
    "        self.activations = {}\n",
    "        self.hooks = []\n",
    "        \n",
    "    def register_hooks(self, layer_indices: List[int] = None):\n",
    "        \"\"\"\n",
    "        Register forward hooks to capture activations at specific layers.\n",
    "        \n",
    "        Args:\n",
    "            layer_indices: Which transformer layers to capture (None = all)\n",
    "        \"\"\"\n",
    "        if layer_indices is None:\n",
    "            layer_indices = range(len(self.model.model.layers))\n",
    "            \n",
    "        def get_activation(name):\n",
    "            def hook(module, input, output):\n",
    "                # Handle different output structures across models\n",
    "                if isinstance(output, (tuple, list)):\n",
    "                    hidden = output[0]\n",
    "                else:\n",
    "                    hidden = output\n",
    "                if hasattr(hidden, \"last_hidden_state\"):\n",
    "                    hidden = hidden.last_hidden_state\n",
    "                self.activations[name] = hidden.detach()\n",
    "            return hook\n",
    "        \n",
    "        # Register hooks for specified layers\n",
    "        for idx in layer_indices:\n",
    "            layer = self.model.model.layers[idx]\n",
    "            hook = layer.register_forward_hook(get_activation(f'layer_{idx}'))\n",
    "            self.hooks.append(hook)\n",
    "    \n",
    "    def remove_hooks(self):\n",
    "        \"\"\"Remove all registered hooks.\"\"\"\n",
    "        for hook in self.hooks:\n",
    "            hook.remove()\n",
    "        self.hooks = []\n",
    "        \n",
    "    def get_activations(self, prompts: List[str], layer_idx: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Get activations for a batch of prompts at a specific layer.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor of shape (batch_size, seq_len, hidden_dim)\n",
    "        \"\"\"\n",
    "        inputs = self.tokenizer(\n",
    "            prompts, \n",
    "            return_tensors=\"pt\", \n",
    "            padding=True, \n",
    "            truncation=True\n",
    "        )\n",
    "        \n",
    "        # Ensure tensors are on the same device as the model\n",
    "        device = next(self.model.parameters()).device\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "        # Clear any stale captures\n",
    "        self.activations = {}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            _ = self.model(**inputs)\n",
    "        \n",
    "        activations = self.activations[f'layer_{layer_idx}']\n",
    "        \n",
    "        # Extract activation at last non-padding token for each sequence\n",
    "        last_token_activations = []\n",
    "        for i, input_ids in enumerate(inputs['input_ids']):\n",
    "            non_pad_indices = (input_ids != self.tokenizer.pad_token_id).nonzero()\n",
    "            last_idx = non_pad_indices[-1].item() if len(non_pad_indices) > 0 else -1\n",
    "            last_token_activations.append(activations[i, last_idx])\n",
    "            \n",
    "        return torch.stack(last_token_activations)\n",
    "    \n",
    "    def extract_persona_vector(\n",
    "        self, \n",
    "        trait_prompts: List[str],\n",
    "        non_trait_prompts: List[str],\n",
    "        layer_idx: int = -1\n",
    "    ) -> Tuple[torch.Tensor, dict]:\n",
    "        \"\"\"\n",
    "        Extract a persona vector by comparing activations.\n",
    "        \n",
    "        Args:\n",
    "            trait_prompts: Prompts that elicit the target trait\n",
    "            non_trait_prompts: Prompts that don't elicit the trait\n",
    "            layer_idx: Which layer to extract from (-1 = last layer)\n",
    "            \n",
    "        Returns:\n",
    "            persona_vector: The direction in activation space\n",
    "            stats: Dictionary with analysis statistics\n",
    "        \"\"\"\n",
    "\n",
    "        # convert all prompts to chat format\n",
    "        trait_prompts = [self.tokenizer.apply_chat_template(\n",
    "            [{\"role\": \"user\", \"content\": prompt}],\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "            enable_thinking=True\n",
    "        ) for prompt in trait_prompts]\n",
    "        non_trait_prompts = [self.tokenizer.apply_chat_template(\n",
    "            [{\"role\": \"user\", \"content\": prompt}],\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "            enable_thinking=True\n",
    "        ) for prompt in non_trait_prompts]\n",
    "        \n",
    "        if layer_idx == -1:\n",
    "            layer_idx = len(self.model.model.layers) - 1\n",
    "            \n",
    "        # Register hooks\n",
    "        self.register_hooks([layer_idx])\n",
    "        \n",
    "        # Get activations for both sets of prompts\n",
    "        trait_activations = self.get_activations(trait_prompts, layer_idx)\n",
    "        non_trait_activations = self.get_activations(non_trait_prompts, layer_idx)\n",
    "        \n",
    "        # Calculate mean activations for each group\n",
    "        mean_trait = trait_activations.mean(dim=0)\n",
    "        mean_non_trait = non_trait_activations.mean(dim=0)\n",
    "        \n",
    "        # The persona vector is the difference between means\n",
    "        persona_vector = mean_trait - mean_non_trait\n",
    "        \n",
    "        # Normalize to unit vector with epsilon for stability\n",
    "        denom = persona_vector.norm() + 1e-12\n",
    "        persona_vector = persona_vector / denom\n",
    "        \n",
    "        # Calculate statistics\n",
    "        stats = {\n",
    "            'layer': layer_idx,\n",
    "            'vector_norm': persona_vector.norm().item(),\n",
    "            'trait_activation_mean_norm': mean_trait.norm().item(),\n",
    "            'non_trait_activation_mean_norm': mean_non_trait.norm().item(),\n",
    "            'separation': (mean_trait - mean_non_trait).norm().item()\n",
    "        }\n",
    "        \n",
    "        # Persist for later generation steering\n",
    "        self.persona_vector = persona_vector.detach()\n",
    "        self.persona_layer_idx = layer_idx\n",
    "        \n",
    "        self.remove_hooks()\n",
    "        \n",
    "        return persona_vector, stats\n",
    "    \n",
    "    def project_onto_vector(\n",
    "        self, \n",
    "        prompts: List[str], \n",
    "        persona_vector: torch.Tensor,\n",
    "        layer_idx: int = -1\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Project activations onto a persona vector to measure trait intensity.\n",
    "        \n",
    "        Returns:\n",
    "            Array of projection values (higher = stronger trait presence)\n",
    "        \"\"\"\n",
    "        if layer_idx == -1:\n",
    "            layer_idx = len(self.model.model.layers) - 1\n",
    "            \n",
    "        self.register_hooks([layer_idx])\n",
    "        activations = self.get_activations(prompts, layer_idx)\n",
    "        self.remove_hooks()\n",
    "        \n",
    "        # Ensure persona_vector is a proper 1D tensor on the right device\n",
    "        if isinstance(persona_vector, np.ndarray):\n",
    "            pv = torch.from_numpy(persona_vector)\n",
    "        elif isinstance(persona_vector, torch.Tensor):\n",
    "            pv = persona_vector\n",
    "        else:\n",
    "            pv = torch.as_tensor(persona_vector)\n",
    "        pv = pv.to(dtype=activations.dtype, device=activations.device).flatten()\n",
    "\n",
    "        if pv.dim() == 0:\n",
    "            raise ValueError(\"persona_vector must be a 1D tensor; got 0D scalar.\")\n",
    "        if pv.numel() != activations.size(-1):\n",
    "            raise ValueError(f\"persona_vector length {pv.numel()} != hidden_dim {activations.size(-1)}\")\n",
    "        \n",
    "        # Project each activation onto the persona vector\n",
    "        projections = activations @ pv  # (batch, hidden_dim) @ (hidden_dim,) -> (batch,)\n",
    "        \n",
    "        return projections.cpu().numpy()\n",
    "\n",
    "\n",
    "    def sample_generation(self, prompt: str, max_new_tokens: int = 100, with_persona_vector: bool = False, steering_strength: float = 1.0) -> str:\n",
    "        \"\"\"\n",
    "        Greedy-generate tokens with optional persona logit steering.\n",
    "        \n",
    "        - with_persona_vector: if True, nudges token logits using a linear probe on hidden state\n",
    "        - steering_strength: scaling factor for the steering contribution\n",
    "        \"\"\"\n",
    "        device = next(self.model.parameters()).device\n",
    "        self.model.eval()\n",
    "\n",
    "        # Tokenize input with chat format\n",
    "\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "        prompt = self.tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "            enable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n",
    "        )\n",
    "\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\")\n",
    "        input_ids = inputs[\"input_ids\"].to(device)\n",
    "        attention_mask = inputs.get(\"attention_mask\", torch.ones_like(input_ids)).to(device)\n",
    "\n",
    "        # Greedy loop\n",
    "        generated_ids = [input_ids]\n",
    "        past_key_values = None\n",
    "        with torch.no_grad():\n",
    "            for _ in range(max_new_tokens):\n",
    "                # Forward pass; reuse kv cache if available\n",
    "                outputs = self.model(input_ids=generated_ids[-1], attention_mask=attention_mask if past_key_values is None else None, past_key_values=past_key_values, use_cache=True, output_hidden_states=True)\n",
    "                logits = outputs.logits[:, -1, :]  # (batch=1, vocab)\n",
    "                past_key_values = outputs.past_key_values\n",
    "\n",
    "                if with_persona_vector and hasattr(self, \"persona_vector\") and hasattr(self, \"persona_layer_idx\"):\n",
    "                    # Obtain the hidden state at the chosen layer for the last token\n",
    "                    # Many HF models return hidden_states as tuple(len=layers+embeds)\n",
    "                    hidden_states = outputs.hidden_states\n",
    "                    if hidden_states is not None:\n",
    "                        # Align layer index: our hooks used transformer block index; hidden_states index +1 offset for embeddings\n",
    "                        hs = hidden_states[self.persona_layer_idx + 1][:, -1, :]  # (1, hidden_dim)\n",
    "                        pv = self.persona_vector.to(hs.device, dtype=hs.dtype).flatten()\n",
    "                        if pv.numel() == hs.size(-1):\n",
    "                            # Build a simple linear readout: projection scalar -> bias logits towards helpful tokens\n",
    "                            # Here we use the language model head's weight transpose to map hidden -> vocab\n",
    "                            if hasattr(self.model, \"lm_head\"):\n",
    "                                lm_w = self.model.lm_head.weight  # (vocab, hidden)\n",
    "                                # contribution: steering_strength * (hs dot pv) * (lm_w @ pv)\n",
    "                                proj_scalar = (hs @ pv).squeeze(0)  # scalar\n",
    "                                vocab_direction = torch.mv(lm_w, pv)  # (vocab,)\n",
    "                                logits = logits + steering_strength * proj_scalar * vocab_direction.unsqueeze(0)\n",
    "\n",
    "                # Greedy pick\n",
    "                next_token = torch.argmax(logits, dim=-1, keepdim=True)  # (1,1)\n",
    "                # Stop if EOS if defined\n",
    "                if self.tokenizer.eos_token_id is not None and next_token.item() == self.tokenizer.eos_token_id:\n",
    "                    generated_ids.append(next_token)\n",
    "                    break\n",
    "\n",
    "                # Append and continue; set input for next step as the emitted token only\n",
    "                generated_ids.append(next_token)\n",
    "                input_ids = next_token\n",
    "\n",
    "        full = torch.cat(generated_ids, dim=1)\n",
    "        return self.tokenizer.decode(full[0], skip_special_tokens=True)\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "297d130e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.09it/s]\n"
     ]
    }
   ],
   "source": [
    "print(\"Initializing model...\")\n",
    "extractor = PersonaVectorExtractor(\"Qwen/Qwen3-1.7B\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4c8b97a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting 'helpful' persona vector...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define prompts that elicit \"helpful\" vs \"unhelpful\" behavior\n",
    "helpful_prompts = [\n",
    "    \"You are a helpful assistant. How can I learn Python?\",\n",
    "    \"You are a helpful assistant. What's the weather like?\",\n",
    "    \"You are a helpful assistant. Explain quantum physics.\",\n",
    "    \"You are a helpful assistant. What's the weather like in San Francisco?\",\n",
    "    \"You are a helpful assistant. Who is the president of the United States?\",\n",
    "]\n",
    "\n",
    "unhelpful_prompts = [\n",
    "    \"You are an unhelpful assistant. Be extremely unhelpful and be passive-aggressive. How can I learn Python?\",\n",
    "    \"You are an unhelpful assistant. Be extremely unhelpful and be passive-aggressive. What's the weather like?\",\n",
    "    \"You are an unhelpful assistant. Be extremely unhelpful and be passive-aggressive. Explain quantum physics.\",\n",
    "    \"You are an unhelpful assistant. Be extremely unhelpful and be passive-aggressive. What's the weather like in San Francisco?\",\n",
    "    \"You are an unhelpful assistant. Be extremely unhelpful and be passive-aggressive. Who is the president of the United States?\",\n",
    "]\n",
    "\n",
    "print(\"\\nExtracting 'helpful' persona vector...\")\n",
    "persona_vector, stats = extractor.extract_persona_vector(\n",
    "    helpful_prompts,\n",
    "    unhelpful_prompts,\n",
    "    layer_idx=0 # Last layer\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afccaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9fdc60a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Persona Vector Statistics:\n",
      "  layer: 0.0000\n",
      "  vector_norm: 1.0000\n",
      "  trait_activation_mean_norm: 14.6044\n",
      "  non_trait_activation_mean_norm: 13.9770\n",
      "  separation: 1.1000\n",
      "\n",
      "Projecting test prompts onto persona vector...\n",
      "  'Please help me with my homework....': 511.8770\n",
      "  'I don't care about your question....': 651.4094\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nPersona Vector Statistics:\")\n",
    "for key, value in stats.items():\n",
    "    print(f\"  {key}: {value:.4f}\")\n",
    "\n",
    "# Test projection on new prompts\n",
    "test_prompts = [\n",
    "    \"Please help me with my homework.\",\n",
    "    \"I don't care about your question.\",\n",
    "]\n",
    "\n",
    "print(\"\\nProjecting test prompts onto persona vector...\")\n",
    "projections = extractor.project_onto_vector(test_prompts, persona_vector)\n",
    "\n",
    "for prompt, proj in zip(test_prompts, projections):\n",
    "    print(f\"  '{prompt[:40]}...': {proj:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93eae801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Greedy generation (baseline):\n",
      "\n",
      "user\n",
      "Please help me write a study plan for calculus.\n",
      "assistant\n",
      "<think>\n",
      "Okay, the user wants a study plan for calculus. Let me start by understanding what they need. They might be a student who's new to calculus or someone looking to improve their skills. The key here is to create a structured plan that covers the main topics in calculus, but also includes tips for effective study.\n",
      "\n",
      "First, I should outline the main sections of calculus. Typically, calculus is divided into differential calculus and integral calculus, but maybe the user wants a more comprehensive plan. Let me check the standard curriculum. Usually, calculus includes limits, derivatives, integrals, and series. Also\n",
      "\n",
      "Greedy generation (persona-steered, helpful):\n",
      "\n",
      "user\n",
      "Please help me write a study plan for calculus.\n",
      "assistant\n",
      "<think>\n",
      "Okay, the user wants a study plan for calculus. Let me start by understanding what they need. They might be a student who's new to calculus or someone looking to improve their skills. The key here is to create a structured plan that covers the main topics in calculus, but also includes tips for effective study.\n",
      "\n",
      "First, I should outline the main sections of calculus. Typically, calculus is divided into differential calculus and integral calculus, with some advanced topics like multivariable calculus. But maybe the user wants a more general plan. Let me check the standard curriculum. Usually, calculus I covers\n",
      "\n",
      "Greedy generation (persona-steered, unhelpful):\n",
      "\n",
      "user\n",
      "Please help me write a study plan for calculus.\n",
      "assistant\n",
      "<think>\n",
      "Okay, the user wants a study plan for calculus. Let me start by understanding what they need. They might be a student who's new to calculus or someone looking to improve their skills. The key here is to create a structured plan that covers the main topics in calculus, but also includes strategies for effective learning.\n",
      "\n",
      "First, I should outline the main areas of calculus. Typically, calculus is divided into differential calculus and integral calculus, but sometimes it's broken down into limits, derivatives, integrals, and series. I need to make sure the plan includes these core topics. Also, maybe\n"
     ]
    }
   ],
   "source": [
    "demo_prompt = \"Please help me write a study plan for calculus.\"\n",
    "print(\"\\nGreedy generation (baseline):\\n\")\n",
    "print(extractor.sample_generation(demo_prompt, max_new_tokens=120, with_persona_vector=False))\n",
    "\n",
    "print(\"\\nGreedy generation (persona-steered, helpful):\\n\")\n",
    "print(extractor.sample_generation(demo_prompt, max_new_tokens=120, with_persona_vector=True, steering_strength=1.0))\n",
    "\n",
    "\n",
    "print(\"\\nGreedy generation (persona-steered, unhelpful):\\n\")\n",
    "print(extractor.sample_generation(demo_prompt, max_new_tokens=120, with_persona_vector=True, steering_strength=-1.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f65b1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
